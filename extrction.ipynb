{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SsIPy6UvcMpICMFDZ0vPJjCdU1I0_iWT","authorship_tag":"ABX9TyP+7pDyq1Zrp8h965nyDJpL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This script was written by Reddit users ***u/Watchful1***, dedicated to efficiently extrct desired information from historically dumped Reddit submissions and comments scraped using Pushshift API before Reddit changed their data sharing policy, and can be fully accessed via: https://github.com/Watchful1/PushshiftDumps/blob/master/scripts/filter_file.py.\n","\n","See more instructions and clarifications for this script: https://www.reddit.com/r/pushshift/comments/11ef9if/separate_dump_files_for_the_top_20k_subreddits/"],"metadata":{"id":"cwQfm31qtvia"}},{"cell_type":"code","source":["!pip install zstandard\n","\n","import zstandard\n","import os\n","import json\n","import sys\n","import csv\n","from datetime import datetime\n","import logging.handlers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf1cxH_uhJIk","executionInfo":{"status":"ok","timestamp":1700597650190,"user_tz":300,"elapsed":8008,"user":{"displayName":"Yeyu Qing","userId":"00818031151919416029"}},"outputId":"422373cd-a036-42f1-9873-644be355c224"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting zstandard\n","  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: zstandard\n","Successfully installed zstandard-0.22.0\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWrEP8jCg7SS","executionInfo":{"status":"ok","timestamp":1700600584419,"user_tz":300,"elapsed":16965,"user":{"displayName":"Yeyu Qing","userId":"00818031151919416029"}},"outputId":"914a87cb-8c7c-43ef-97a0-ac996be6bca6"},"outputs":[{"output_type":"stream","name":"stderr","text":["2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","2023-11-21 21:02:48,407 - INFO: Filtering field: title\n","INFO:bot:Filtering field: title\n","2023-11-21 21:02:48,426 - INFO: On values: \n","2023-11-21 21:02:48,426 - INFO: On values: \n","2023-11-21 21:02:48,426 - INFO: On values: \n","2023-11-21 21:02:48,426 - INFO: On values: \n","2023-11-21 21:02:48,426 - INFO: On values: \n","2023-11-21 21:02:48,426 - INFO: On values: \n","INFO:bot:On values: \n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","2023-11-21 21:02:48,438 - INFO: Exact match off. Single field None.\n","INFO:bot:Exact match off. Single field None.\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,445 - INFO: From date 2019-01-01 to date 2021-12-31\n","INFO:bot:From date 2019-01-01 to date 2021-12-31\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","2023-11-21 21:02:48,459 - INFO: Output format set to csv\n","INFO:bot:Output format set to csv\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","2023-11-21 21:02:48,470 - INFO: Processing 1 files\n","INFO:bot:Processing 1 files\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:48,481 - INFO: Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","INFO:bot:Input: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst : Output: /content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021.csv : Is submission True\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:02:53,484 - INFO: 2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","INFO:bot:2018-08-21 03:24:12 : 100,000 : 0 : 0 : 30,933,700:54%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:00,879 - INFO: 2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","INFO:bot:2021-10-08 09:34:52 : 200,000 : 87,105 : 0 : 50,332,800:89%\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","2023-11-21 21:03:05,224 - INFO: Complete : 240,149 : 98,662 : 0\n","INFO:bot:Complete : 240,149 : 98,662 : 0\n"]}],"source":["# put the path to the input file, or a folder of files to process all of\n","input_file = r\"/content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions.zst\"\n","# put the name or path to the output file. The file extension from below will be added automatically. If the input file is a folder, the output will be treated as a folder as well\n","output_file = r\"/content/drive/MyDrive/ESE 546 Principle of Deep Learning/Project/ethereum_submissions_2019-2021\"\n","# the format to output in, pick from the following options\n","#   zst: same as the input, a zstandard compressed ndjson file. Can be read by the other scripts in the repo\n","#   txt: an ndjson file, which is a text file with a separate json object on each line. Can be opened by any text editor\n","#   csv: a comma separated value file. Can be opened by a text editor or excel\n","# WARNING READ THIS: if you use txt or csv output on a large input file without filtering out most of the rows, the resulting file will be extremely large. Usually about 7 times as large as the compressed input file\n","output_format = \"csv\"\n","# override the above format and output only this field into a text file, one per line. Useful if you want to make a list of authors or ids. See the examples below\n","# any field that's in the dump is supported, but useful ones are\n","#   author: the username of the author\n","#   id: the id of the submission or comment\n","#   link_id: only for comments, the fullname of the submission the comment is associated with\n","#   parent_id: only for comments, the fullname of the parent of the comment. Either another comment or the submission if it's top level\n","single_field = None\n","# the fields in the file are different depending on whether it has comments or submissions. If we're writing a csv, we need to know which fields to write.\n","# set this to true to write out to the log every time there's a bad line, set to false if you're expecting only some of the lines to match the key\n","write_bad_lines = True\n","\n","# only output items between these two dates\n","from_date = datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n","to_date = datetime.strptime(\"2021-12-31\", \"%Y-%m-%d\")\n","\n","# the field to filter on, the values to filter with and whether it should be an exact match\n","# some examples:\n","#\n","# return only objects where the author is u/watchful1 or u/spez\n","# field = \"author\"\n","# values = [\"watchful1\",\"spez\"]\n","# exact_match = True\n","#\n","# return only objects where the title contains either \"stonk\" or \"moon\"\n","# field = \"title\"\n","# values = [\"stonk\",\"moon\"]\n","# exact_match = False\n","#\n","# return only objects where the body contains either \"stonk\" or \"moon\". For submissions the body is in the \"selftext\" field, for comments it's in the \"body\" field\n","# field = \"selftext\"\n","# values = [\"stonk\",\"moon\"]\n","# exact_match = False\n","#\n","#\n","# filter a submission file and then get a file with all the comments only in those submissions. This is a multi step process\n","# add your submission filters and set the output file name to something unique\n","# input_file = \"redditdev_submissions.zst\"\n","# output_file = \"filtered_submissions\"\n","# output_format = \"csv\"\n","# field = \"author\"\n","# values = [\"watchful1\"]\n","#\n","# run the script, this will result in a file called \"filtered_submissions.csv\" that contains only submissions by u/watchful1\n","# now we'll run the script again with the same input and filters, but set the output to single field. Be sure to change the output file to a new name, but don't change any of the other inputs\n","# output_file = \"submission_ids\"\n","# single_field = \"id\"\n","#\n","# run the script again, this will result in a file called \"submission_ids.txt\" that has an id on each line\n","# now we'll remove all the other filters and update the script to input from the comments file, and use the submission ids list we created before. And change the output name again so we don't override anything\n","# input_file = \"redditdev_comments.zst\"\n","# output_file = \"filtered_comments\"\n","# single_field = None  # resetting this back so it's not used\n","# field = \"link_id\"  # in the comment object, this is the field that contains the submission id\n","# values_file = \"submission_ids.txt\"\n","# exact_match = False  # the link_id field has a prefix on it, so we can't do an exact match\n","#\n","# run the script one last time and now you have a file called \"filtered_comments.csv\" that only has comments from your submissions above\n","# if you want only top level comments instead of all comments, you can set field to \"parent_id\" instead of \"link_id\"\n","\n","field = \"title\"\n","values = ['']\n","# if you have a long list of values, you can put them in a file and put the filename here. If set this overrides the value list above\n","# if this list is very large, it could greatly slow down the process\n","values_file = None\n","exact_match = False\n","\n","# sets up logging to the console as well as a file\n","log = logging.getLogger(\"bot\")\n","log.setLevel(logging.INFO)\n","log_formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')\n","log_str_handler = logging.StreamHandler()\n","log_str_handler.setFormatter(log_formatter)\n","log.addHandler(log_str_handler)\n","if not os.path.exists(\"logs\"):\n","\tos.makedirs(\"logs\")\n","log_file_handler = logging.handlers.RotatingFileHandler(os.path.join(\"logs\", \"bot.log\"), maxBytes=1024*1024*16, backupCount=5)\n","log_file_handler.setFormatter(log_formatter)\n","log.addHandler(log_file_handler)\n","\n","\n","def write_line_zst(handle, line):\n","\thandle.write(line.encode('utf-8'))\n","\thandle.write(\"\\n\".encode('utf-8'))\n","\n","\n","def write_line_json(handle, obj):\n","\thandle.write(json.dumps(obj))\n","\thandle.write(\"\\n\")\n","\n","\n","def write_line_single(handle, obj, field):\n","\tif field in obj:\n","\t\thandle.write(obj[field])\n","\telse:\n","\t\tlog.info(f\"{field} not in object {obj['id']}\")\n","\thandle.write(\"\\n\")\n","\n","\n","def write_line_csv(writer, obj, is_submission):\n","\toutput_list = []\n","\toutput_list.append(str(obj['score']))\n","\toutput_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n","\tif is_submission:\n","\t\toutput_list.append(obj['title'])\n","\toutput_list.append(f\"u/{obj['author']}\")\n","\toutput_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n","\tif is_submission:\n","\t\tif obj['is_self']:\n","\t\t\tif 'selftext' in obj:\n","\t\t\t\toutput_list.append(obj['selftext'])\n","\t\t\telse:\n","\t\t\t\toutput_list.append(\"\")\n","\t\telse:\n","\t\t\toutput_list.append(obj['url'])\n","\telse:\n","\t\toutput_list.append(obj['body'])\n","\twriter.writerow(output_list)\n","\n","\n","def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n","\tchunk = reader.read(chunk_size)\n","\tbytes_read += chunk_size\n","\tif previous_chunk is not None:\n","\t\tchunk = previous_chunk + chunk\n","\ttry:\n","\t\treturn chunk.decode()\n","\texcept UnicodeDecodeError:\n","\t\tif bytes_read > max_window_size:\n","\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n","\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n","\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n","\n","\n","def read_lines_zst(file_name):\n","\twith open(file_name, 'rb') as file_handle:\n","\t\tbuffer = ''\n","\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n","\t\twhile True:\n","\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n","\n","\t\t\tif not chunk:\n","\t\t\t\tbreak\n","\t\t\tlines = (buffer + chunk).split(\"\\n\")\n","\n","\t\t\tfor line in lines[:-1]:\n","\t\t\t\tyield line.strip(), file_handle.tell()\n","\n","\t\t\tbuffer = lines[-1]\n","\n","\t\treader.close()\n","\n","\n","def process_file(input_file, output_file, output_format, field, values, from_date, to_date, single_field, exact_match):\n","\toutput_path = f\"{output_file}.{output_format}\"\n","\tis_submission = \"submission\" in input_file\n","\tlog.info(f\"Input: {input_file} : Output: {output_path} : Is submission {is_submission}\")\n","\twriter = None\n","\tif output_format == \"zst\":\n","\t\thandle = zstandard.ZstdCompressor().stream_writer(open(output_path, 'wb'))\n","\telif output_format == \"txt\":\n","\t\thandle = open(output_path, 'w', encoding='UTF-8')\n","\telif output_format == \"csv\":\n","\t\thandle = open(output_path, 'w', encoding='UTF-8', newline='')\n","\t\twriter = csv.writer(handle, escapechar='\\\\')\n","\telse:\n","\t\tlog.error(f\"Unsupported output format {output_format}\")\n","\t\tsys.exit()\n","\n","\tfile_size = os.stat(input_file).st_size\n","\tcreated = None\n","\tmatched_lines = 0\n","\tbad_lines = 0\n","\ttotal_lines = 0\n","\tfor line, file_bytes_processed in read_lines_zst(input_file):\n","\t\ttotal_lines += 1\n","\t\tif total_lines % 100000 == 0:\n","\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {total_lines:,} : {matched_lines:,} : {bad_lines:,} : {file_bytes_processed:,}:{(file_bytes_processed / file_size) * 100:.0f}%\")\n","\n","\t\ttry:\n","\t\t\tobj = json.loads(line)\n","\t\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n","\n","\t\t\tif created < from_date:\n","\t\t\t\tcontinue\n","\t\t\tif created > to_date:\n","\t\t\t\tcontinue\n","\n","\t\t\tif field is not None:\n","\t\t\t\tfield_value = obj[field].lower()\n","\t\t\t\tmatched = False\n","\t\t\t\tfor value in values:\n","\t\t\t\t\tif exact_match:\n","\t\t\t\t\t\tif value == field_value:\n","\t\t\t\t\t\t\tmatched = True\n","\t\t\t\t\t\t\tbreak\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tif value in field_value:\n","\t\t\t\t\t\t\tmatched = True\n","\t\t\t\t\t\t\tbreak\n","\t\t\t\tif not matched:\n","\t\t\t\t\tcontinue\n","\n","\t\t\tmatched_lines += 1\n","\t\t\tif output_format == \"zst\":\n","\t\t\t\twrite_line_zst(handle, line)\n","\t\t\telif output_format == \"csv\":\n","\t\t\t\twrite_line_csv(writer, obj, is_submission)\n","\t\t\telif output_format == \"txt\":\n","\t\t\t\tif single_field is not None:\n","\t\t\t\t\twrite_line_single(handle, obj, single_field)\n","\t\t\t\telse:\n","\t\t\t\t\twrite_line_json(handle, obj)\n","\t\t\telse:\n","\t\t\t\tlog.info(f\"Something went wrong, invalid output format {output_format}\")\n","\t\texcept (KeyError, json.JSONDecodeError) as err:\n","\t\t\tbad_lines += 1\n","\t\t\tif write_bad_lines:\n","\t\t\t\tif isinstance(err, KeyError):\n","\t\t\t\t\tlog.warning(f\"Key {field} is not in the object: {err}\")\n","\t\t\t\telif isinstance(err, json.JSONDecodeError):\n","\t\t\t\t\tlog.warning(f\"Line decoding failed: {err}\")\n","\t\t\t\tlog.warning(line)\n","\n","\thandle.close()\n","\tlog.info(f\"Complete : {total_lines:,} : {matched_lines:,} : {bad_lines:,}\")\n","\n","\n","if __name__ == \"__main__\":\n","\tif single_field is not None:\n","\t\tlog.info(\"Single field output mode, changing output file format to txt\")\n","\t\toutput_format = \"txt\"\n","\n","\tif values_file is not None:\n","\t\tvalues = []\n","\t\twith open(values_file, 'r') as values_handle:\n","\t\t\tfor value in values_handle:\n","\t\t\t\tvalues.append(value.strip().lower())\n","\t\tlog.info(f\"Loaded {len(values)} from values file {values_file}\")\n","\telse:\n","\t\tvalues = [value.lower() for value in values]  # convert to lowercase\n","\n","\tlog.info(f\"Filtering field: {field}\")\n","\tif len(values) <= 20:\n","\t\tlog.info(f\"On values: {','.join(values)}\")\n","\telse:\n","\t\tlog.info(f\"On values:\")\n","\t\tfor value in values:\n","\t\t\tlog.info(value)\n","\tlog.info(f\"Exact match {('on' if exact_match else 'off')}. Single field {single_field}.\")\n","\tlog.info(f\"From date {from_date.strftime('%Y-%m-%d')} to date {to_date.strftime('%Y-%m-%d')}\")\n","\tlog.info(f\"Output format set to {output_format}\")\n","\n","\tinput_files = []\n","\tif os.path.isdir(input_file):\n","\t\tif not os.path.exists(output_file):\n","\t\t\tos.makedirs(output_file)\n","\t\tfor file in os.listdir(input_file):\n","\t\t\tif not os.path.isdir(file) and file.endswith(\".zst\"):\n","\t\t\t\tinput_name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n","\t\t\t\tinput_files.append((os.path.join(input_file, file), os.path.join(output_file, input_name)))\n","\telse:\n","\t\tinput_files.append((input_file, output_file))\n","\tlog.info(f\"Processing {len(input_files)} files\")\n","\tfor file_in, file_out in input_files:\n","\t\tprocess_file(file_in, file_out, output_format, field, values, from_date, to_date, single_field, exact_match)"]}]}